
#!/usr/bin/env python3
# Update NetBox CFs from CSV (emoji summary + numeric fields).
# Preserve ALL existing tags and ensure 'ip-availables-sync' is present.
# Uses netbox_prefix_id when present in CSV; else matches by CIDR.

import os, sys, csv, argparse, ipaddress
import pynetbox

CF_SUMMARY_NAME = "list_available_ips"
CF_USED_NAME = "ips_used"
CF_AVAIL_NAME = "ips_available"

CF_SUMMARY_DEF = {
    "name": CF_SUMMARY_NAME, "label": "List available IPs", "type": "text",
    "content_types": ["ipam.prefix"], "description": "Summary of used and available IPs in the prefix",
    "required": False,
}
CF_USED_DEF = {
    "name": CF_USED_NAME, "label": "IP Used", "type": "integer",
    "content_types": ["ipam.prefix"], "description": "Number of IP addresses used in the prefix",
    "required": False,
}
CF_AVAIL_DEF = {
    "name": CF_AVAIL_NAME, "label": "IP Availables", "type": "integer",
    "content_types": ["ipam.prefix"], "description": "Number of IP addresses available in the prefix",
    "required": False,
}

TAG_NAME = "ip-availables-sync"
TAG_SLUG = "ip-availables-sync"
TAG_COLOR = "teal"
CREATE_DESC = os.environ.get("IP_SYNC_CREATE_DESC", "‚ö†Ô∏è PREFIX CREATED BY IP AVAILABILITY SYNC")

def to_int(s, default=0):
    try:
        s = (s or "").replace(" ", "").replace(",", "")
        return int(float(s)) if s else default
    except Exception:
        return default

def to_float(s, default=None):
    try:
        s = (s or "").strip().replace("%", "").replace(",", ".")
        return float(s)
    except Exception:
        return default

def fmt_int(n):
    try: n = int(n)
    except Exception: return "0"
    return f"{n:,}".replace(",", " ")

def avail_pct(prefix_cidr, ips_avail, ipv4_reserved=5):
    try: net = ipaddress.ip_network(prefix_cidr, strict=False)
    except Exception: return None
    if net.version == 6: return None
    usable = max(net.num_addresses - ipv4_reserved, 0)
    if usable == 0: return None
    return round(max(0.0, min(100.0, (float(ips_avail)/float(usable))*100.0))), 1)

def status_emoji(pct, green_th=60.0, orange_th=30.0):
    if pct is None: return "üîµ"
    if pct >= green_th: return "üü¢"
    if pct >= orange_th: return "üü†"
    return "üî¥"

def make_summary(prefix_cidr, nb_subnets, ips_used, ips_avail, green_th, orange_th):
    try:
        net = ipaddress.ip_network(prefix_cidr, strict=False)
    except Exception:
        net = None
    pct = None
    if net and net.version == 4:
        total = net.num_addresses
        usable = max(total - 5, 0)
        pct = round((ips_avail / usable) * 100.0, 1) if usable else None
    status = status_emoji(pct, green_th, orange_th)
    line = f"{status} | üß© Subnets: {fmt_int(nb_subnets)} | üî¥ Used: {fmt_int(ips_used)} | üü¢ Available: {fmt_int(ips_avail)}"
    if pct is not None: line += f" | ‚öñÔ∏è {pct}%"
    return line

def find_col(fieldnames, candidates):
    lower = {(fn or "").strip().lower(): fn for fn in fieldnames}
    for c in candidates:
        if c in lower: return lower[c]
    return None

def ensure_cf(nb, cf_def):
    try:
        name = cf_def["name"]
        if list(nb.extras.custom_fields.filter(name=name)): return
        nb.extras.custom_fields.create(cf_def)
    except Exception as e:
        print(f"[WARN] ensure_cf {cf_def.get('name')}: {e}", file=sys.stderr)

def ensure_tag(nb):
    try:
        found = list(nb.extras.tags.filter(slug=TAG_SLUG))
        return found[0] if found else nb.extras.tags.create({"name": TAG_NAME, "slug": TAG_SLUG, "color": TAG_COLOR})
    except Exception as e:
        print(f"[WARN] ensure_tag {TAG_SLUG}: {e}", file=sys.stderr)
        return None

def build_tags_payload(nb, prefix_obj, sync_tag_id, sync_tag_slug):
    """Return a list suitable for PATCH 'tags': keep existing tags + add sync tag.
       Prefer numeric IDs; fall back to {'slug': ...} or {'name': ...} if needed.
       Does NOT call /extras/tags to resolve IDs (works with limited permissions)."""
    payload = []

    def add_tag_entry(tag):
        # tag may be dict/int/str from API representation
        if isinstance(tag, dict):
            if tag.get("id") is not None:
                payload.append(tag["id"])
            elif tag.get("slug"):
                payload.append({"slug": tag["slug"]})
            elif tag.get("name"):
                payload.append({"name": tag["name"]})
        elif isinstance(tag, int):
            payload.append(tag)
        elif isinstance(tag, str):
            # unknown form, assume slug
            payload.append({"slug": tag})

    # Fresh copy to ensure we see current tags
    fresh = None
    try:
        fresh = nb.ipam.prefixes.get(prefix_obj.id)
    except Exception:
        fresh = None
    tags_src = getattr(fresh, "tags", None) if fresh is not None else getattr(prefix_obj, "tags", None)

    for t in (tags_src or []):
        add_tag_entry(t)

    # Ensure sync tag present
    if sync_tag_id is not None:
        # check if already present (id or dict with id)
        def has_id(id_):
            for x in payload:
                if isinstance(x, int) and x == id_: return True
                if isinstance(x, dict) and x.get("id") == id_: return True
            return False
        if not has_id(sync_tag_id):
            payload.append(sync_tag_id)
    else:
        # fallback to slug
        def has_slug(slug):
            for x in payload:
                if isinstance(x, dict) and x.get("slug") == slug: return True
            return False
        if not has_slug(sync_tag_slug):
            payload.append({"slug": sync_tag_slug})

    # Dedupe (by id first, else by slug/name tuple or int)
    seen = set()
    deduped = []
    def key(x):
        if isinstance(x, int): return ("id", x)
        if isinstance(x, dict):
            if "id" in x: return ("id", x["id"])
            if "slug" in x: return ("slug", x["slug"])
            if "name" in x: return ("name", x["name"])
        return ("raw", repr(x))
    for x in payload:
        k = key(x)
        if k not in seen:
            seen.add(k)
            deduped.append(x)
    return deduped

def iter_matches(nb, prefix):
    try:
        for obj in nb.ipam.prefixes.filter(prefix=prefix):
            yield obj
    except Exception as e:
        print(f"[ERR] Query failed for prefix={prefix}: {e}", file=sys.stderr)

def main():
    ap = argparse.ArgumentParser(description="Update NetBox CFs from CSV; preserve all tags and add 'ip-availables-sync'.")
    ap.add_argument("csv", help="CSV (supports netbox_prefix_id column)")
    ap.add_argument("--green-th", type=float, default=None, help="üü¢ threshold (%% available). Default env AVAIL_GREEN_TH or 60")
    ap.add_argument("--orange-th", type=float, default=None, help="üü† threshold (%% available). Default env AVAIL_ORANGE_TH or 30")
    ap.add_argument("--strict-unique", action="store_true", help="Update only if match is unique (ignored when ID present).")
    ap.add_argument("--create-missing", action="store_true", help="Create container prefixes (global) when missing and tag them.")
    ap.add_argument("--no-create-cf", action="store_true", help="Do not create CFs if missing.")
    ap.add_argument("--dry-run", action="store_true", help="Print actions; do not write.")
    args = ap.parse_args()

    NETBOX_URL = os.environ.get("NETBOX_URL")
    NETBOX_TOKEN = os.environ.get("NETBOX_TOKEN")
    if not NETBOX_URL or not NETBOX_TOKEN:
        print("Error: set NETBOX_URL and NETBOX_TOKEN", file=sys.stderr); sys.exit(2)

    green_th = to_float(os.environ.get("AVAIL_GREEN_TH"), 60.0)
    orange_th = to_float(os.environ.get("AVAIL_ORANGE_TH"), 30.0)
    if args.green_th is not None: green_th = args.green_th
    if args.orange_th is not None: orange_th = args.orange_th
    if green_th < orange_th:
        print(f"[WARN] green-th ({green_th}) < orange-th ({orange_th}) -> swapping", file=sys.stderr)
        green_th, orange_th = orange_th, green_th

    nb = pynetbox.api(NETBOX_URL, token=NETBOX_TOKEN)

    if not args.no_create_cf:
        ensure_cf(nb, CF_SUMMARY_DEF); ensure_cf(nb, CF_USED_DEF); ensure_cf(nb, CF_AVAIL_DEF)
    tag_obj = ensure_tag(nb); tag_id = getattr(tag_obj, "id", None)

    with open(args.csv, "r", encoding="utf-8-sig", newline="") as f:
        sample = f.read(4096); f.seek(0)
        try: dialect = csv.Sniffer().sniff(sample, delimiters=",;")
        except Exception: dialect = csv.excel
        reader = csv.DictReader(f, dialect=dialect)

        cols = reader.fieldnames or []
        col_prefix = find_col(cols, {"address space","adresse space","prefix"})
        col_subnets = find_col(cols, {"subnets","nb subnets","nombre de subnets","nb_subnets"})
        col_used = find_col(cols, {"ips used","ips utilis√©es","ips utilisees","ips_used","used"})
        col_avail = find_col(cols, {"ips available","ips disponibles","ips disponible","ips_available","available"})
        col_id = find_col(cols, {"netbox_prefix_id","netbox prefix id"})
        if not all([col_prefix, col_subnets, col_used, col_avail]):
            print("[ERR] Missing columns. Need: 'address space'/'prefix', 'subnets', 'ips used', 'ips available'", file=sys.stderr)
            print("Headers:", cols, file=sys.stderr); sys.exit(1)

        updated = skipped = multi = missing = created = 0

        for row in reader:
            prefix = (row.get(col_prefix) or "").strip()
            if not prefix: continue
            nb_subnets = to_int(row.get(col_subnets), 0)
            ips_used   = to_int(row.get(col_used), 0)
            ips_avail  = to_int(row.get(col_avail), 0)
            summary = make_summary(prefix, nb_subnets, ips_used, ips_avail, green_th, orange_th)

            # Resolve match: prefer ID if present
            matches = []
            if col_id:
                id_val = (row.get(col_id) or "").strip()
                if id_val.isdigit():
                    try:
                        obj = nb.ipam.prefixes.get(int(id_val))
                        if obj: matches = [obj]
                    except Exception as e:
                        print(f"[ERR] Lookup by ID failed for {prefix} (id={id_val}): {e}", file=sys.stderr)
            if not matches:
                matches = [obj for obj in iter_matches(nb, prefix)]

            if not matches and args.create_missing:
                payload = {"prefix": prefix, "status": "container", "description": CREATE_DESC}
                if tag_id is not None: payload["tags"] = [tag_id]
                else: payload["tags"] = [{"slug": TAG_SLUG}]
                if args.dry_run:
                    print(f"[DRY][CREATE] container {prefix} with tag '{TAG_SLUG}'")
                else:
                    try:
                        newp = nb.ipam.prefixes.create(payload)
                        print(f"[CREATE] {prefix} id={newp.id}")
                        matches = [newp]; created += 1
                    except Exception as e:
                        print(f"[ERR] Create failed for {prefix}: {e}", file=sys.stderr)

            if not matches:
                print(f"[MISS] Prefix not found in NetBox: {prefix}"); missing += 1; continue
            if args.strict_unique and len(matches) != 1:
                print(f"[SKIP] Non-unique ({len(matches)}) for {prefix}"); skipped += 1; continue

            for p in matches:
                tags_payload = build_tags_payload(nb, p, tag_id, TAG_SLUG)
                cf = p.custom_fields or {}
                cf[CF_SUMMARY_NAME] = summary
                cf[CF_USED_NAME] = ips_used
                cf[CF_AVAIL_NAME] = ips_avail

                if args.dry_run:
                    print(f"[DRY][UPDATE] {prefix}: tags={tags_payload} | {CF_SUMMARY_NAME}='{summary}' | {CF_USED_NAME}={ips_used} | {CF_AVAIL_NAME}={ips_avail}")
                else:
                    try:
                        ok = p.update({"custom_fields": cf, "tags": tags_payload})
                        if ok:
                            print(f"[OK] Updated {prefix} (tags preserved + sync tag ensured)")
                            updated += 1
                        else:
                            print(f"[ERR] Update failed for {prefix}", file=sys.stderr)
                    except Exception as e:
                        print(f"[ERR] Update exception for {prefix}: {getattr(e, 'error', None) or e}", file=sys.stderr)

            if not args.strict_unique and len(matches) > 1: multi += 1

    print(f"\nSummary: updated={updated}, created={created}, skipped={skipped}, multi-prefixes={multi}, missing={missing}")

if __name__ == "__main__":
    main()











#!/bin/bash

# Create directory
mkdir -p netbox-azure-bootstrap
cd netbox-azure-bootstrap

# README.md
cat <<'EOF' > README.md
# NetBox Azure Bootstrap Scripts

This package helps you extract Azure subscription and network information, and push it into NetBox.

## Scripts

- `azure_to_netbox_sites.sh` ‚Äî Creates NetBox Sites for each Azure subscription (including subscription ID in a custom field).
- `azure_to_netbox_prefixes.sh` ‚Äî Pushes all Azure VNet prefixes into NetBox, attached to the correct site.
- `bulk_create_aggregates.sh` ‚Äî Bulk import IP aggregates from a CSV into NetBox.

## Usage

1. Edit each script to set your NetBox URL and API token.
2. Ensure you have `az`, `jq`, and `curl` installed and authenticated.
3. Run scripts as needed.

See each script for details.
EOF

# azure_to_netbox_sites.sh
cat <<'EOF' > azure_to_netbox_sites.sh
#!/bin/bash

NETBOX_URL="https://your-netbox-instance/api"
NETBOX_TOKEN="YOUR_NETBOX_API_TOKEN"

# Helper function to POST to NetBox
netbox_post() {
    local endpoint="$1"
    local data="$2"
    curl -s -X POST "${NETBOX_URL}${endpoint}/" \
        -H "Authorization: Token ${NETBOX_TOKEN}" \
        -H "Content-Type: application/json" \
        -d "${data}"
}

for SUB in $(az account list --all --query '[].id' -o tsv); do
    az account set --subscription "$SUB"
    SUB_NAME=$(az account show --query 'name' -o tsv)
    SITE_SLUG=$(echo "$SUB_NAME" | tr '[:upper:]' '[:lower:]' | tr ' ' '-')

    # Build the JSON payload with custom field for subscription ID
    SITE_DATA=$(jq -n \
      --arg name "$SUB_NAME" \
      --arg slug "$SITE_SLUG" \
      --arg subid "$SUB" \
      '{name: $name, slug: $slug, custom_fields: {azure_subscription_id: $subid}}'
    )

    netbox_post "/dcim/sites" "$SITE_DATA"
    echo "Added site: $SUB_NAME ($SUB)"
done
EOF

# azure_to_netbox_prefixes.sh
cat <<'EOF' > azure_to_netbox_prefixes.sh
#!/bin/bash

NETBOX_URL="https://your-netbox-instance/api"
NETBOX_TOKEN="YOUR_NETBOX_API_TOKEN"

netbox_post() {
    local endpoint="$1"
    local data="$2"
    curl -s -X POST "${NETBOX_URL}${endpoint}/" \
        -H "Authorization: Token ${NETBOX_TOKEN}" \
        -H "Content-Type: application/json" \
        -d "${data}"
}

for SUB_ID in $(az account list --all --query '[].id' -o tsv); do
  az account set --subscription "$SUB_ID"
  SUB_NAME=$(az account show --query 'name' -o tsv)
  SITE_SLUG=$(echo "$SUB_NAME" | tr '[:upper:]' '[:lower:]' | tr ' ' '-')

  # Fetch the Site ID by slug
  SITE_ID=$(curl -s -X GET "${NETBOX_URL}/dcim/sites/?slug=$SITE_SLUG" \
              -H "Authorization: Token ${NETBOX_TOKEN}" | jq '.results[0].id')

  for VNET in $(az network vnet list --query '[].id' -o tsv); do
    VNET_JSON=$(az network vnet show --ids "$VNET")
    VNET_NAME=$(echo "$VNET_JSON" | jq -r '.name')
    for PREFIX in $(echo "$VNET_JSON" | jq -r '.addressSpace.addressPrefixes[]'); do
      PREFIX_DATA="{\"prefix\": \"$PREFIX\", \"site\": $SITE_ID, \"description\": \"VNet $VNET_NAME in $SUB_NAME\"}"
      netbox_post "/ipam/prefixes" "$PREFIX_DATA"
    done
  done
done
EOF

# bulk_create_aggregates.sh
cat <<'EOF' > bulk_create_aggregates.sh
#!/bin/bash

NETBOX_URL="https://your-netbox-instance/api"
NETBOX_TOKEN="YOUR_NETBOX_API_TOKEN"
CSV_FILE="aggregates.csv"

# Helper to get RIR ID by slug
get_rir_id() {
    local slug=$1
    curl -s -H "Authorization: Token $NETBOX_TOKEN" "$NETBOX_URL/ipam/rirs/?slug=$slug" | jq '.results[0].id'
}

while IFS=, read -r prefix rir_slug; do
    # Skip header
    if [[ "$prefix" == "prefix" ]]; then continue; fi

    rir_id=$(get_rir_id "$rir_slug")
    if [[ -z "$rir_id" || "$rir_id" == "null" ]]; then
        echo "RIR not found for slug $rir_slug, skipping $prefix"
        continue
    fi

    data="{\"prefix\": \"$prefix\", \"rir\": $rir_id}"
    curl -s -X POST "$NETBOX_URL/ipam/aggregates/" \
        -H "Authorization: Token $NETBOX_TOKEN" \
        -H "Content-Type: application/json" \
        -d "$data"
    echo "Created aggregate $prefix (RIR: $rir_slug)"
done < "$CSV_FILE"
EOF

# aggregates.csv
cat <<'EOF' > aggregates.csv
prefix,rir_slug
10.0.0.0/8,rfc1918
172.16.0.0/12,rfc1918
192.168.0.0/16,rfc1918
EOF

# notifications.txt
cat <<'EOF' > notifications.txt
- Make sure you have 'azure_subscription_id' custom field created in NetBox for Sites.
- Scripts require az, jq, and curl.
- For large tenants, consider running scripts during off-hours.
- Review NetBox API permissions.
EOF

# Make scripts executable
chmod +x *.sh

# Go up, zip the folder
cd ..
zip -r netbox-azure-bootstrap.zip netbox-azure-bootstrap

echo "All done! Your netbox-azure-bootstrap.zip is ready."
