```python
#!/usr/bin/env python3

import os
import sys
import logging
import argparse
from pathlib import Path
import yaml
import re
import ipaddress
from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, ClientSecretCredential
from azure.mgmt.subscription import SubscriptionClient
from azure.mgmt.network import NetworkManagementClient
from azure.mgmt.managementgroups import ManagementGroupsAPI
from pynetbox import api
from pynetbox.core.query import RequestError
import requests

# Default logging config (can be overridden by YAML or CLI)
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_config_yaml(config_path='config.yaml'):
    """Load configuration from YAML file if it exists"""
    config = {}
    config_file = Path(config_path)
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                config = yaml.safe_load(f)
            logger.info(f"Loaded configuration from {config_path}")
        except yaml.YAMLError as e:
            logger.error(f"Error parsing {config_path}: {str(e)}")
        except Exception as e:
            logger.error(f"Error loading {config_path}: {str(e)}")
    else:
        logger.info(f"No {config_path} found; using defaults and CLI args")
    return config

def get_azure_credentials(config, args):
    """Get Azure credentials based on config and args"""
    auth_method = args.auth_method or config.get('azure', {}).get('authentication', {}).get('method', 'default')
    
    client_id = args.azure_client_id or config.get('azure', {}).get('authentication', {}).get('client_id')
    client_secret = args.azure_client_secret or config.get('azure', {}).get('authentication', {}).get('client_secret')
    tenant_id = args.azure_tenant_id or config.get('azure', {}).get('authentication', {}).get('tenant_id')
    
    if client_id and client_secret and tenant_id:
        logger.info("Using Azure Service Principal authentication")
        return ClientSecretCredential(
            tenant_id=tenant_id,
            client_id=client_id,
            client_secret=client_secret
        )
    elif auth_method == 'interactive':
        logger.info("Using interactive browser authentication for Azure")
        return InteractiveBrowserCredential()
    else:
        logger.info("Using default Azure credential chain")
        return DefaultAzureCredential()

def get_management_group_subscriptions(credential, management_group_id=None, management_group_name=None):
    logger.info("Getting subscriptions from management group")
    
    try:
        mg_client = ManagementGroupsAPI(credential)
        
        if management_group_name and not management_group_id:
            logger.info(f"Looking for management group with name: {management_group_name}")
            management_groups = mg_client.management_groups.list()
            for mg in management_groups:
                if mg.display_name == management_group_name:
                    management_group_id = mg.name
                    logger.info(f"Found management group ID: {management_group_id}")
                    break
            
            if not management_group_id:
                logger.error(f"Management group with name '{management_group_name}' not found")
                return []
        
        mg_details = mg_client.management_groups.get(
            group_id=management_group_id,
            expand="children",
            recurse=True
        )
        
        subscriptions = []
        
        def extract_subscriptions(mg_node):
            if hasattr(mg_node, 'children') and mg_node.children:
                for child in mg_node.children:
                    if child.type == "/subscriptions":
                        subscription_info = type('obj', (object,), {
                            'subscription_id': child.name,
                            'display_name': child.display_name
                        })
                        subscriptions.append(subscription_info)
                        logger.info(f"Found subscription: {child.display_name} ({child.name})")
                    elif child.type == "/providers/Microsoft.Management/managementGroups":
                        extract_subscriptions(child)
        
        extract_subscriptions(mg_details)
        
        logger.info(f"Found {len(subscriptions)} subscriptions in management group")
        return subscriptions
        
    except Exception as e:
        logger.error(f"Error getting subscriptions from management group: {str(e)}")
        return []

def get_azure_subscriptions(credential):
    logger.info("Getting Azure subscriptions")
    subscription_client = SubscriptionClient(credential)
    subscriptions = list(subscription_client.subscriptions.list())
    logger.info(f"Found {len(subscriptions)} subscriptions")
    return subscriptions

def get_vnets_and_subnets(subscription_id, credential):
    logger.info(f"Getting VNets for subscription {subscription_id}")
    network_client = NetworkManagementClient(credential, subscription_id)
    
    vnets = list(network_client.virtual_networks.list_all())
    logger.info(f"Found {len(vnets)} VNets in subscription {subscription_id}")
    
    vnet_data = []
    for vnet in vnets:
        address_spaces = [prefix for prefix in vnet.address_space.address_prefixes if prefix]  # Skip empty
        logger.debug(f"VNet {vnet.name} has {len(address_spaces)} address spaces: {address_spaces}")
        vnet_info = {
            'name': vnet.name,
            'id': vnet.id,
            'resource_group': vnet.id.split('/')[4],
            'location': vnet.location,
            'address_space': address_spaces
        }
        vnet_data.append(vnet_info)
    
    return vnet_data

def get_vnet_peerings(network_client, resource_group, vnet_name, config):
    """Fetch peerings for a VNet and format the 'Peering check' string"""
    peering_config = config.get('peering', {})
    if not peering_config.get('enabled', True):
        return "Peering check disabled"
    
    ok_value = peering_config.get('ok_value', '✅')  # Updated default to emoji
    ko_value = peering_config.get('ko_value', '❌')  # Updated default to emoji
    no_peerings = peering_config.get('no_peerings_value', 'No peerings')
    
    try:
        peerings = list(network_client.virtual_network_peerings.list(resource_group, vnet_name))
        if not peerings:
            return no_peerings
        
        peering_strs = []
        for peering in peerings:
            sync_status = peering.peering_sync_level if hasattr(peering, 'peering_sync_level') else 'Unknown'
            state = peering.peering_state if hasattr(peering, 'peering_state') else 'Unknown'
            
            logger.debug(f"Raw sync_status for {peering.name}: {sync_status}, state: {state}")  # Log raw values
            
            sync_ok = ok_value if sync_status == 'FullyInSync' else ko_value
            state_ok = ok_value if state == 'Connected' else ko_value
            
            peering_strs.append(f"name: {peering.name} - {sync_status} {sync_ok} - Peering state {state} {state_ok}")
        
        return "; ".join(peering_strs)
    except Exception as e:
        logger.warning(f"Error fetching peerings for VNet {vnet_name}: {str(e)}")
        return "Error fetching peerings"

def get_or_create_rir(nb, name, slug, description, is_private=False, tags=None):
    try:
        rir = nb.ipam.rirs.get(slug=slug)
        if rir:
            logger.info(f"Found existing RIR: {name}")
            return rir
    except Exception as e:
        logger.debug(f"Error getting RIR {name}: {str(e)}")
    
    logger.info(f"Creating new RIR: {name}")
    tag_dicts = tags or []
    return nb.ipam.rirs.create(
        name=name,
        slug=slug,
        description=description,
        is_private=is_private,
        tags=tag_dicts
    )

def get_or_create_aggregate(nb, prefix, rir_id, description, tags):
    try:
        aggregate = nb.ipam.aggregates.get(prefix=prefix)
        if aggregate:
            logger.info(f"Found existing aggregate: {prefix}")
            return aggregate
    except Exception as e:
        logger.debug(f"Error getting aggregate {prefix}: {str(e)}")
    
    logger.info(f"Creating new aggregate: {prefix}")
    return nb.ipam.aggregates.create(
        prefix=prefix,
        rir=rir_id,
        description=description,
        tags=tags
    )

def clean_color(color_str, default="aaaaaa"):
    """Clean and validate a color string (strip #, ensure 6 hex digits)"""
    if not isinstance(color_str, str):
        logger.warning(f"Invalid color type: {type(color_str)}; using default {default}")
        return default
    color_str = color_str.strip().lstrip('#').lower()
    if re.match(r'^[0-9a-f]{6}$', color_str):
        return color_str
    logger.warning(f"Invalid color format '{color_str}'; using default {default}")
    return default

def get_or_create_tag(nb, tag_input, tag_description="", tag_color="aaaaaa"):
    if isinstance(tag_input, dict):
        logger.debug("Tag input is a dict; extracting properties.")
        tag_name = tag_input.get('name')
        if not tag_name:
            logger.error("Tag dict missing 'name' key; skipping this tag.")
            return None  # Skip instead of raising to prevent crash
        tag_description = tag_input.get('description', tag_description)
        tag_color = clean_color(tag_input.get('color', tag_color))
    elif isinstance(tag_input, str):
        tag_name = tag_input
        tag_color = clean_color(tag_color)  # Use passed or default
    else:
        logger.error(f"Invalid tag_input type: {type(tag_input)}. Must be str or dict; skipping.")
        return None

    slug = tag_name.lower().replace(" ", "-")
    try:
        tag = nb.extras.tags.get(slug=slug)
        if tag:
            needs_update = False
            if tag.description != tag_description:
                tag.description = tag_description
                needs_update = True
            if tag.color != tag_color:
                tag.color = tag_color
                needs_update = True
            if needs_update:
                tag.save()
                logger.info(f"Updated existing tag: {tag_name} (color: {tag_color})")
            else:
                logger.info(f"Found existing tag: {tag_name} (color: {tag.color})")
            return tag
    except Exception as e:
        logger.debug(f"Error getting tag {tag_name}: {str(e)}")
    
    logger.info(f"Creating new tag: {tag_name} (color: {tag_color})")
    try:
        return nb.extras.tags.create(
            name=tag_name,
            slug=slug,
            description=tag_description,
            color=tag_color
        )
    except Exception as e:
        logger.error(f"Failed to create tag {tag_name}: {str(e)}")
        return None

def get_or_create_ipam_role(nb, name, slug, description, tags):
    try:
        role = nb.ipam.roles.get(slug=slug)
        if role:
            logger.info(f"Found existing IPAM role: {name}")
            return role
    except Exception as e:
        logger.debug(f"Error getting IPAM role {name}: {str(e)}")
    
    logger.info(f"Creating new IPAM role: {name}")
    tag_dicts = [t for t in tags if isinstance(t, dict) and 'id' in t]  # Filter valid dicts
    return nb.ipam.roles.create(
        name=name,
        slug=slug,
        description=description,
        tags=tag_dicts
    )

def get_or_create_tenant_group(nb, name, slug, description=""):
    try:
        group = nb.tenancy.tenant_groups.get(slug=slug)
        if group:
            logger.info(f"Found existing Tenant Group: {name}")
            return group
    except Exception as e:
        logger.debug(f"Error getting Tenant Group {name}: {str(e)}")
    
    logger.info(f"Creating new Tenant Group: {name}")
    return nb.tenancy.tenant_groups.create(
        name=name,
        slug=slug,
        description=description
    )

def get_or_create_tenant(nb, name, group_id, description="", tags=None):
    slug = name.lower().replace(' ', '-')  # Generate slug from name (UUID is already suitable)
    try:
        tenant = nb.tenancy.tenants.get(slug=slug)
        if tenant:
            needs_update = False
            if tenant.group.id != group_id:
                tenant.group = group_id
                needs_update = True
            if tenant.description != description:
                tenant.description = description
                needs_update = True
            if tags and set(tag.id for tag in tenant.tags) != set(tag['id'] for tag in tags):
                tenant.tags = tags
                needs_update = True
            if needs_update:
                tenant.save()
                logger.info(f"Updated Tenant: {name}")
            else:
                logger.info(f"Found existing Tenant: {name}")
            return tenant
    except RequestError as e:
        logger.debug(f"Request error getting Tenant {name}: {str(e)}")
    except Exception as e:
        logger.debug(f"Error getting Tenant {name}: {str(e)}")
    
    logger.info(f"Creating new Tenant: {name} with slug {slug}")
    try:
        return nb.tenancy.tenants.create(
            name=name,
            slug=slug,  # Required field
            group=group_id,
            description=description,
            tags=tags or []
        )
    except RequestError as e:
        logger.error(f"Failed to create Tenant {name}: {str(e)}")
        raise

def get_or_create_custom_field(nb, field_name, field_type, field_description, object_types, field_choices=None):
    try:
        custom_field = nb.extras.custom_fields.get(name=field_name)
        if custom_field:
            logger.info(f"Found existing custom field: {field_name}")
            return custom_field
    except Exception as e:
        logger.debug(f"Error getting custom field {field_name}: {str(e)}")

    logger.info(f"Creating new custom field: {field_name}")
    field_data = {
        'name': field_name,
        'label': field_name.replace('_', ' ').title(),
        'type': field_type,
        'description': field_description,
        'object_types': object_types,
        'required': False,
        'default': None
    }
    if field_choices:
        field_data['choices'] = field_choices

    return nb.extras.custom_fields.create(**field_data)

def get_or_create_prefix(nb, prefix_value, defaults, subscription_name=None, subscription_id=None, aggregate_id=None, role_id=None, tenant_id=None, site_id=None, tags=None, status='active', is_pool=False):
    try:
        existing_prefixes = nb.ipam.prefixes.filter(prefix=prefix_value)
        
        if existing_prefixes:
            logger.info(f"Found existing prefix: {prefix_value}")
            prefix = list(existing_prefixes)[0]
            
            needs_update = False
            for key, value in defaults.items():
                if key == 'parent':  
                    continue
                current_value = getattr(prefix, key, None)
                if current_value != value:
                    setattr(prefix, key, value)
                    needs_update = True
            
            if subscription_name and subscription_id:
                custom_fields = getattr(prefix, 'custom_fields', {}) or {}
                azure_subscription_value = subscription_name
                
                existing_value = custom_fields.get('azure_subscription', '')
                if existing_value != azure_subscription_value or re.search(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}', existing_value, re.IGNORECASE):
                    custom_fields['azure_subscription'] = azure_subscription_value
                    custom_fields['azure_subscription_url'] = f"https://portal.azure.com/#@/subscription/{subscription_id}/overview"
                    prefix.custom_fields = custom_fields
                    needs_update = True
                    logger.debug(f"Forced update of azure_subscription to clean value: {azure_subscription_value}")
            
            if aggregate_id and getattr(prefix, 'aggregate', None) != aggregate_id:
                prefix.aggregate = aggregate_id
                needs_update = True
            
            if role_id and getattr(prefix, 'role', None) != role_id:
                prefix.role = role_id
                needs_update = True
            
            if tenant_id and getattr(prefix, 'tenant', None) != tenant_id:
                prefix.tenant = tenant_id
                needs_update = True
            
            if site_id and getattr(prefix, 'site', None) != site_id:
                prefix.site = site_id
                needs_update = True
            
            if tags and set(tag.id for tag in prefix.tags) != set(tag['id'] for tag in tags):
                prefix.tags = tags
                needs_update = True
            
            if prefix.status != status:
                prefix.status = status
                needs_update = True
            
            if prefix.is_pool != is_pool:
                prefix.is_pool = is_pool
                needs_update = True
            
            if needs_update:
                prefix.save()
                logger.info(f"Updated prefix: {prefix_value}")
                
            return prefix, False
    except AttributeError as e:
        logger.error(f"Attribute error when updating prefix {prefix_value}: {str(e)}")
    except Exception as e:
        logger.debug(f"Error checking for existing prefix {prefix_value}: {str(e)}")
    
    try:
        logger.info(f"Creating new prefix: {prefix_value}")
        
        if subscription_name and subscription_id:
            if 'custom_fields' not in defaults:
                defaults['custom_fields'] = {}
            defaults['custom_fields']['azure_subscription'] = subscription_name
            defaults['custom_fields']['azure_subscription_url'] = f"https://portal.azure.com/#@/subscription/{subscription_id}/overview"
        
        if aggregate_id:
            defaults['aggregate'] = aggregate_id
        if role_id:
            defaults['role'] = role_id
        if tenant_id:
            defaults['tenant'] = tenant_id
        if site_id:
            defaults['site'] = site_id
        if tags:
            defaults['tags'] = tags
        
        defaults['status'] = status
        defaults['is_pool'] = is_pool
        
        return nb.ipam.prefixes.create(
            prefix=prefix_value,
            **defaults
        ), True
    except RequestError as e:
        if "Duplicate prefix found" in str(e):
            logger.warning(f"Duplicate prefix found: {prefix_value}. Attempting to retrieve existing prefix.")
            try:
                existing_prefixes = nb.ipam.prefixes.filter(prefix=prefix_value)
                if existing_prefixes:
                    prefix = list(existing_prefixes)[0]
                    logger.info(f"Retrieved existing prefix: {prefix_value}")
                    return prefix, False
            except Exception as inner_e:
                logger.error(f"Error retrieving duplicate prefix {prefix_value}: {str(inner_e)}")
        raise

def setup_rirs_and_aggregates(nb, config):
    rirs_config = config.get('rirs', [])
    aggregates_config = config.get('aggregates', [])
    
    rir_map = {}
    for rir in rirs_config:
        rir_tags = []
        for tag_item in rir.get('tags', []):
            tag = get_or_create_tag(nb, tag_item)
            if tag:
                rir_tags.append({'id': tag.id})
        
        created_rir = get_or_create_rir(
            nb,
            name=rir.get('name'),
            slug=rir.get('slug'),
            description=rir.get('description', ''),
            is_private=rir.get('is_private', False),
            tags=rir_tags
        )
        rir_map[rir['name']] = created_rir.id
    
    aggregate_map = {}
    for agg in aggregates_config:
        rir_id = rir_map.get(agg.get('rir'))
        if not rir_id:
            logger.error(f"RIR {agg.get('rir')} not found for aggregate {agg.get('prefix')}")
            continue
        
        agg_tags = []
        agg_tag_names = []  # Store lowercase tag names for filtering
        for tag_item in agg.get('tags', []):
            tag = get_or_create_tag(nb, tag_item)
            if tag:
                agg_tags.append({'id': tag.id})
                agg_tag_names.append(tag.name.lower())
        
        created_agg = get_or_create_aggregate(
            nb,
            prefix=agg.get('prefix'),
            rir_id=rir_id,
            description=agg.get('description', ''),
            tags=agg_tags
        )
        aggregate_map[agg.get('prefix')] = {
            'id': created_agg.id,
            'tags': agg_tags,
            'tag_names': agg_tag_names,  # For efficient filter checks
            'rir_id': rir_id
        }
    
    return aggregate_map

def setup_ipam_roles(nb, config):
    roles_config = config.get('ipam_roles', [])
    role_map = {}
    
    for role in roles_config:
        role_tags = []
        for tag_item in role.get('tags', []):
            tag = get_or_create_tag(nb, tag_item)
            if tag:
                role_tags.append({'id': tag.id})
        
        created_role = get_or_create_ipam_role(
            nb,
            name=role.get('name'),
            slug=role.get('slug'),
            description=role.get('description', ''),
            tags=role_tags
        )
        role_map[role['name']] = {
            'id': created_role.id,
            'tags': role_tags,
            'match_criteria': role.get('match_criteria', '')
        }
    
    return role_map

def setup_tenancy(nb, config, all_network_data):
    tenancy_config = config.get('tenancy', {})
    groups_config = tenancy_config.get('groups', [])  # Note: Your config has 'group' (singular), but I assume it's a list; adjust if needed
    
    group_map = {}
    for group in groups_config:
        created_group = get_or_create_tenant_group(
            nb,
            name=group.get('name'),
            slug=group.get('slug'),
            description=group.get('description', '')
        )
        group_map[group['name']] = created_group.id
    
    default_group_name = tenancy_config.get('default_group', 'AZURE')
    default_group_id = group_map.get(default_group_name)
    if not default_group_id:
        logger.error(f"Default Tenant Group {default_group_name} not found")
        return {}, ''
    
    tenant_tag_names = tenancy_config.get('tenant_tags', [])
    tenant_tags = []
    for t in tenant_tag_names:
        tag = get_or_create_tag(nb, t)
        if tag:
            tenant_tags.append({'id': tag.id})
    
    tenant_map = {}
    for sub_data in all_network_data:
        sub_id = sub_data['subscription_id']
        sub_name = sub_data['subscription_name']
        
        created_tenant = get_or_create_tenant(
            nb,
            name=sub_id,
            group_id=default_group_id,
            description=sub_name,
            tags=tenant_tags
        )
        tenant_map[sub_id] = {
            'id': created_tenant.id,
            'tags': tenant_tags
        }
        logger.info(f"Processed Tenant for subscription {sub_id}: {sub_name}")
    
    return tenant_map, tenancy_config.get('aggregate_tag_filter', '')

def setup_custom_fields(nb, config):
    logger.info("Setting up custom fields for Azure integration")
    custom_fields_config = config.get('custom_fields', {})
    
    try:
        if custom_fields_config.get('azure_subscription', {}).get('enabled', True):
            get_or_create_custom_field(
                nb,
                field_name="azure_subscription",
                field_type=custom_fields_config.get('azure_subscription', {}).get('field_type', 'text'),
                field_description=custom_fields_config.get('azure_subscription', {}).get('description', "Azure subscription name"),
                object_types=["ipam.prefix"]
            )
        
        if custom_fields_config.get('azure_subscription_url', {}).get('enabled', True):
            get_or_create_custom_field(
                nb,
                field_name="azure_subscription_url",
                field_type=custom_fields_config.get('azure_subscription_url', {}).get('field_type', 'url'),
                field_description=custom_fields_config.get('azure_subscription_url', {}).get('description', "Direct link to Azure subscription portal"),
                object_types=["ipam.prefix"]
            )
        
        # New: Peering check custom field (use valid name without space)
        peering_config = config.get('peering', {})
        if peering_config.get('enabled', True):
            get_or_create_custom_field(
                nb,
                field_name="peering_check",  # Valid name: lowercase with underscore
                field_type="text",
                field_description="VNet peering status summary",
                object_types=["ipam.prefix"]
            )
        
        logger.info("Custom fields setup completed")
    except Exception as e:
        logger.error(f"Error setting up custom fields: {str(e)}")
        raise  # Raise to stop if creation fails

def setup_custom_tags(nb, config):
    """Create or update custom tags from config.yaml, including adding/updating colors"""
    custom_tags = config.get('custom_tags', [])
    if not custom_tags:
        logger.info("No custom_tags defined in config.yaml; skipping")
        return

    logger.info("Setting up custom tags from config.yaml")
    for tag_config in custom_tags:
        if not isinstance(tag_config, dict):
            logger.warning(f"Skipping invalid custom tag config: {tag_config} (must be dict)")
            continue
        name = tag_config.get('name')
        if not name:
            logger.warning("Skipping custom tag without name")
            continue

        slug = tag_config.get('slug', name.lower().replace(" ", "-"))
        description = tag_config.get('description', "")
        color = clean_color(tag_config.get('color', "aaaaaa"))

        tag = get_or_create_tag(nb, {'name': name, 'description': description, 'color': color})
        if not tag:
            logger.warning(f"Failed to process custom tag '{name}'")

    logger.info("Custom tags setup completed")

def get_or_create_site_group(nb, name, slug=None, description=""):
    slug = slug or name.lower().replace(" ", "-")
    try:
        group = nb.dcim.site_groups.get(slug=slug)
        if group:
            if group.description != description:
                group.description = description
                group.save()
                logger.info(f"Updated Site Group: {name}")
            else:
                logger.info(f"Found existing Site Group: {name}")
            return group
    except Exception as e:
        logger.debug(f"Error getting Site Group {name}: {str(e)}")
    
    logger.info(f"Creating new Site Group: {name}")
    return nb.dcim.site_groups.create(
        name=name,
        slug=slug,
        description=description
    )

def get_or_create_region(nb, name, slug, tags=None, description=""):
    try:
        region = nb.dcim.regions.get(slug=slug)
        if region:
            needs_update = False
            if region.name != name:
                region.name = name
                needs_update = True
            if region.description != description:
                region.description = description
                needs_update = True
            if tags and set(tag.id for tag in region.tags) != set(tag['id'] for tag in tags):
                region.tags = tags
                needs_update = True
            if needs_update:
                region.save()
                logger.info(f"Updated Region: {name} (slug: {slug})")
            else:
                logger.info(f"Found existing Region: {name} (slug: {slug})")
            return region
    except Exception as e:
        logger.debug(f"Error getting Region {name}: {str(e)}")
    
    logger.info(f"Creating new Region: {name} (slug: {slug})")
    return nb.dcim.regions.create(
        name=name,
        slug=slug,
        description=description,
        tags=tags or []
    )

def get_or_create_site(nb, name, slug, group_id=None, region_id=None, tenant_id=None, tags=None, status='active', description=""):
    try:
        site = nb.dcim.sites.get(slug=slug)
        if site:
            needs_update = False
            if site.name != name:
                site.name = name
                needs_update = True
            if site.group and site.group.id != group_id:
                site.group = group_id
                needs_update = True
            if site.region and site.region.id != region_id:
                site.region = region_id
                needs_update = True
            if tenant_id and (not site.tenant or site.tenant.id != tenant_id):
                site.tenant = tenant_id
                needs_update = True
            if site.status != status:
                site.status = status
                needs_update = True
            if site.description != description:
                site.description = description
                needs_update = True
            if tags and set(tag.id for tag in site.tags) != set(tag['id'] for tag in tags):
                site.tags = tags
                needs_update = True
            if needs_update:
                site.save()
                logger.info(f"Updated Site: {name} (slug: {slug})")
            else:
                logger.info(f"Found existing Site: {name} (slug: {slug})")
            return site
    except Exception as e:
        logger.debug(f"Error getting Site {name}: {str(e)}")
    
    logger.info(f"Creating new Site: {name} (slug: {slug})")
    site_data = {
        'name': name,
        'slug': slug,
        'status': status,
        'description': description,
        'tags': tags or []
    }
    if group_id:
        site_data['group'] = group_id
    if region_id:
        site_data['region'] = region_id
    if tenant_id:
        site_data['tenant'] = tenant_id
    return nb.dcim.sites.create(**site_data)

def setup_organization(nb, config):
    org_config = config.get('organization', {})
    if not org_config:
        logger.info("No organization config found; skipping setup")
        return None, {}

    # Setup Site Group
    site_group_config = org_config.get('site_group', {})
    site_group = get_or_create_site_group(
        nb,
        name=site_group_config.get('name', 'Azure Sites'),
        description=site_group_config.get('description', 'Group for all Azure-related sites')
    )
    site_group_id = site_group.id if site_group else None

    # Setup Regions and Tags from mapping (now a list of dicts)
    regions_config = org_config.get('regions', {})
    mappings = regions_config.get('mapping', [])  # List of dicts
    region_tag_config = regions_config.get('region_tags', {})
    default_tag_color = region_tag_config.get('default_color', '0000ff')  # Blue default
    desc_prefix = region_tag_config.get('default_description_prefix', 'Azure Region: ')

    region_map = {}
    for entry in mappings:
        if not isinstance(entry, dict):
            logger.warning(f"Skipping invalid region mapping entry: {entry} (must be dict)")
            continue
        
        azure_location = entry.get('azure_location')
        tag_name = entry.get('tag_name')
        tag_color = clean_color(entry.get('color', default_tag_color))  # Per-region color or default
        
        if not azure_location or not tag_name:
            logger.warning(f"Skipping region mapping missing azure_location or tag_name: {entry}")
            continue

        # Derive human-readable name (e.g., "francecentral" -> "France Central")
        human_name = ' '.join(word.capitalize() for word in re.split(r'(?=[A-Z])', azure_location) if word)

        # Create/Get Tag (e.g., "FRC" with specific color)
        tag_desc = f"{desc_prefix}{human_name}"
        tag = get_or_create_tag(nb, tag_name, tag_description=tag_desc, tag_color=tag_color)
        tag_dict = [{'id': tag.id}] if tag else []

        # Create/Get Region (slug = lowercase tag_name, e.g., "frc")
        region_slug = tag_name.lower()
        region = get_or_create_region(
            nb,
            name=human_name,
            slug=region_slug,
            tags=tag_dict,
            description=tag_desc
        )
        if region:
            region_map[azure_location] = {
                'id': region.id,
                'human_name': human_name,
                'tag': tag_dict
            }
            logger.info(f"Processed Region for {azure_location}: {human_name} (slug: {region_slug}) with tag {tag_name} (color: {tag_color})")

    return site_group_id, region_map

def setup_static_prefixes(nb, config):
    """Setup static prefixes and their children from config.yaml, and collect child tags for inheritance"""
    prefixes_config = config.get('static_prefixes', [])
    if not prefixes_config:
        logger.info("No static_prefixes defined in config.yaml; skipping")
        return []

    logger.info("Setting up static prefixes from config.yaml")

    child_tag_map = []  # List of (child_network, child_tags) for later inheritance

    def create_prefix_recursive(prefix_config, parent_id=None):
        prefix_value = prefix_config.get('prefix')
        if not prefix_value:
            logger.warning("Skipping prefix config without 'prefix'")
            return None

        # Process tags
        tags = []
        for tag_item in prefix_config.get('tags', []):
            tag = get_or_create_tag(nb, tag_item)
            if tag:
                tags.append({'id': tag.id})

        # Defaults for prefix (with status and is_pool support)
        defaults = {
            'description': prefix_config.get('description', '')
        }

        status = prefix_config.get('status', 'container')  # Default to 'container' for parents
        is_pool = prefix_config.get('is_pool', False)  # Default to false

        # Create or get the prefix (no Azure-specific fields here)
        prefix, created = get_or_create_prefix(
            nb,
            prefix_value,
            defaults,
            tags=tags,
            status=status,
            is_pool=is_pool
        )
        action = "Created" if created else "Updated"
        logger.info(f"{action} static prefix: {prefix_value} (status: {status}, is_pool: {is_pool})")

        # If this is a child (has no children or is leaf), collect for tag inheritance
        if 'children' not in prefix_config or not prefix_config['children']:
            try:
                child_network = ipaddress.ip_network(prefix_value, strict=False)
                child_tag_map.append((child_network, tags))
                logger.debug(f"Collected child prefix for tag inheritance: {prefix_value} with {len(tags)} tags")
            except ValueError as e:
                logger.warning(f"Invalid network for child prefix {prefix_value}: {str(e)}")

        # Recurse for children
        for child_config in prefix_config.get('children', []):
            create_prefix_recursive(child_config, parent_id=prefix.id if prefix else None)

    # Process each top-level prefix
    for prefix_config in prefixes_config:
        create_prefix_recursive(prefix_config)

    logger.info("Static prefixes setup completed")
    return child_tag_map

def apply_inherited_tags(nb, child_tag_map):
    """Apply tags from static child prefixes to all active prefixes that are subnets of them"""
    if not child_tag_map:
        logger.info("No child prefixes for tag inheritance; skipping")
        return

    logger.info("Applying inherited tags to active prefixes")

    # Get all active prefixes
    active_prefixes = nb.ipam.prefixes.filter(status='active')
    for prefix in active_prefixes:
        prefix_net = ipaddress.ip_network(prefix.prefix, strict=False)
        needs_update = False
        current_tags = set(tag.id for tag in prefix.tags)

        for child_net, inherited_tags in child_tag_map:
            if prefix_net.subnet_of(child_net):
                for tag_dict in inherited_tags:
                    if tag_dict['id'] not in current_tags:
                        prefix.tags.append(tag_dict)
                        current_tags.add(tag_dict['id'])
                        needs_update = True
                logger.debug(f"Inherited tags from {child_net} to prefix {prefix.prefix}")

        if needs_update:
            prefix.save()
            logger.info(f"Updated prefix {prefix.prefix} with inherited tags")

    logger.info("Inherited tags application completed")

def sync_to_netbox(all_network_data, netbox_url, netbox_token, config, credential):  # Pass credential
    logger.info(f"Syncing data to Netbox at {netbox_url}")
    
    session = requests.Session()
    session.verify = config.get('ssl', {}).get('verify', True)
    session.timeout = config.get('timeouts', {}).get('netbox_api', 30)
    
    nb = api(netbox_url, token=netbox_token)
    nb.http_session = session
    
    # Global default tag color from config (used for string tags)
    default_tag_color = config.get('tags', {}).get('default_color', 'aaaaaa')
    logger.debug(f"Using global default tag color: {default_tag_color}")
    
    setup_custom_fields(nb, config)
    setup_custom_tags(nb, config)  # Setup custom tags from config.yaml
    
    aggregate_map = setup_rirs_and_aggregates(nb, config)
    role_map = setup_ipam_roles(nb, config)
    tenant_map, aggregate_tag_filter = setup_tenancy(nb, config, all_network_data)
    
    # New: Setup static prefixes and collect child tag map for inheritance
    child_tag_map = setup_static_prefixes(nb, config)
    
    # New: Setup Organization (Sites, Site Groups, Regions)
    site_group_id, region_map = setup_organization(nb, config)
    
    sync_tag_config = config.get('tags', {}).get('sync_tag', {})
    sync_tag_name = sync_tag_config.get('name', "azure-sync")
    sync_tag_desc = sync_tag_config.get('description', "Synced from Azure")
    sync_tag_color = clean_color(sync_tag_config.get('color', default_tag_color))  # Use configured color or fallback
    azure_tag = get_or_create_tag(
        nb,
        tag_input=sync_tag_name,
        tag_description=sync_tag_desc,
        tag_color=sync_tag_color  # Pass the dedicated color
    )
    if not azure_tag:
        logger.error("Failed to create/get azure-sync tag; continuing without it.")
        azure_tag_dict = []
    else:
        azure_tag_dict = [{'id': azure_tag.id}]
    
    additional_tags = []
    additional_tag_configs = config.get('tags', {}).get('additional_tags', [])
    for tag_item in additional_tag_configs:
        tag = get_or_create_tag(nb, tag_item, tag_color=default_tag_color)
        if tag:
            additional_tags.append({'id': tag.id})
    
    for subscription_data in all_network_data:
        subscription_id = subscription_data['subscription_id']
        subscription_name = subscription_data['subscription_name']
        
        matching_role = None
        for role_name, role_data in role_map.items():
            criteria = role_data['match_criteria'].lower()
            if criteria in subscription_name.lower():
                matching_role = role_data
                logger.debug(f"Matched subscription {subscription_name} to role {role_name} using criteria '{criteria}'")
                break
            else:
                logger.debug(f"No match for subscription {subscription_name} with role {role_name} criteria '{criteria}'")
        
        role_id = matching_role['id'] if matching_role else None
        if not matching_role:
            logger.info(f"No role matched for subscription {subscription_name}")
        
        tenant_data = tenant_map.get(subscription_id)
        tenant_id = tenant_data['id'] if tenant_data else None
        
        network_client = NetworkManagementClient(credential, subscription_id)
        
        for vnet in subscription_data['vnets']:
            address_spaces = vnet['address_space']
            if not address_spaces:
                logger.warning(f"Skipping VNet {vnet['name']} (no address spaces)")
                continue
            
            logger.info(f"Processing VNet {vnet['name']} with {len(address_spaces)} address spaces")
            
            peering_check = get_vnet_peerings(network_client, vnet['resource_group'], vnet['name'], config)
            logger.debug(f"Peering check for VNet {vnet['name']}: {peering_check}")
            
            # New: Get/Create Site for this VNet based on location
            vnet_location = vnet['location']
            region_data = region_map.get(vnet_location)
            site_id = None
            if region_data:
                site_prefix = config.get('organization', {}).get('site_prefix', 'Azure - ')
                human_region = region_data['human_name']
                # Updated: Include shortened subscription_id in site name/slug for uniqueness
                short_sub_id = subscription_id[:8]  # First 8 characters of subscription ID
                site_name = f"{site_prefix}{short_sub_id}-{vnet['name']} ({human_region})"
                site_slug = site_name.lower().replace(" ", "-").replace("(", "").replace(")", "")
                site_desc = f"Azure VNet: {vnet['name']} in {human_region} (Subscription: {subscription_id} - {subscription_name})"
                
                site = get_or_create_site(
                    nb,
                    name=site_name,
                    slug=site_slug,
                    group_id=site_group_id,
                    region_id=region_data['id'],
                    tenant_id=tenant_id,  # New: Assign tenant to site
                    tags=region_data['tag'],  # Assign the region-specific tag (e.g., [{'id': FRC_tag_id}])
                    description=site_desc
                )
                site_id = site.id if site else None
                logger.info(f"Processed Site for VNet {vnet['name']}: {site_name} (id: {site_id})")
            else:
                logger.warning(f"No region mapping found for Azure location '{vnet_location}'; skipping Site/Region for VNet {vnet['name']}")
            
            for idx, address_space in enumerate(address_spaces, 1):
                logger.debug(f"Syncing address space {idx}/{len(address_spaces)} for VNet {vnet['name']}: {address_space}")
                
                matching_aggregate = None
                try:
                    prefix_net = ipaddress.ip_network(address_space, strict=False)
                    for agg_prefix, agg_data in aggregate_map.items():
                        agg_net = ipaddress.ip_network(agg_prefix, strict=False)
                        if prefix_net.subnet_of(agg_net):
                            matching_aggregate = agg_data
                            logger.debug(f"Matched address space {address_space} to aggregate {agg_prefix}")
                            break
                except ValueError as e:
                    logger.warning(f"Invalid IP network for {address_space}: {str(e)}")
                    continue
                
                aggregate_id = matching_aggregate['id'] if matching_aggregate else None
                
                # FORCE assign_tenant = True to ensure EVERY prefix gets the tenant (one per subscription)
                # This ensures all prefixes for a subscription are visible under the tenant in NetBox UI.
                assign_tenant = True  # Forced to True (ignores aggregate_tag_filter for tenant assignment)
                logger.debug(f"Forcing tenant assignment for prefix {address_space} (tenant_id: {tenant_id})")
                
                prefix_tags = azure_tag_dict + additional_tags
                if matching_aggregate:
                    prefix_tags += matching_aggregate['tags']
                if matching_role:
                    prefix_tags += matching_role['tags']
                if region_data and region_data['tag']:
                    prefix_tags += region_data['tag']  # Add region tag to prefix
                
                prefix_defaults = {
                    'description': f"Azure VNet: {vnet['name']} (Subscription: {subscription_id}) - Address Space {idx}",
                    'status': 'active',
                    'tags': prefix_tags
                }
                if 'custom_fields' not in prefix_defaults:
                    prefix_defaults['custom_fields'] = {}
                prefix_defaults['custom_fields']['peering_check'] = peering_check
                
                vnet_prefix, created = get_or_create_prefix(
                    nb,
                    address_space,
                    prefix_defaults,
                    subscription_name=subscription_name,
                    subscription_id=subscription_id,
                    aggregate_id=aggregate_id,
                    role_id=role_id,
                    tenant_id=tenant_id if assign_tenant else None,
                    site_id=site_id,
                    tags=prefix_tags
                )
                
                action = "Created" if created else "Updated"
                logger.info(f"{action} prefix for VNet {vnet['name']} address space {idx}: {address_space} (Aggregate: {aggregate_id if aggregate_id else 'None'}, Role: {role_id if role_id else 'None'}, Tenant: {tenant_id if tenant_id else 'None'}, Site: {site_id if site_id else 'None'})")

    # New: After all Azure syncing, apply inherited tags from static child prefixes
    apply_inherited_tags(nb, child_tag_map)

def parse_arguments():
    parser = argparse.ArgumentParser(description='Sync Azure network data to Netbox')
    parser.add_argument('--netbox-url', help='Netbox URL')
    parser.add_argument('--netbox-token', help='Netbox API token')
    parser.add_argument('--auth-method', help='Azure auth method (default, interactive)')
    parser.add_argument('--subscription-id', help='Specific Azure subscription ID to process')
    parser.add_argument('--management-group-id', help='Azure Management Group ID')
    parser.add_argument('--management-group-name', help='Azure Management Group name')
    parser.add_argument('--azure-client-id', help='Azure Service Principal Client ID (appId)')
    parser.add_argument('--azure-client-secret', help='Azure Service Principal Client Secret')
    parser.add_argument('--azure-tenant-id', help='Azure Tenant ID')
    parser.add_argument('--config', default='config.yaml', help='Path to config YAML file')
    return parser.parse_args()

def main():
    args = parse_arguments()
    
    config = load_config_yaml(args.config)
    
    netbox_url = args.netbox_url or config.get('netbox', {}).get('url')
    netbox_token = args.netbox_token or config.get('netbox', {}).get('token')
    
    if not netbox_url or not netbox_token:
        logger.error("Netbox URL and token must be provided either as arguments or in config.yaml")
        sys.exit(1)
    
    log_level = config.get('logging', {}).get('level', 'DEBUG')
    log_format = config.get('logging', {}).get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logging.basicConfig(level=log_level, format=log_format)
    
    filters = config.get('filters', {})
    if filters.get('regions', {}).get('include'):
        logger.warning("Region include filters set but not implemented yet")
    
    try:
        logger.info("Starting Azure to Netbox sync")
        
        credential = get_azure_credentials(config, args)
        
        process_all = config.get('azure', {}).get('subscriptions', {}).get('process_all', True)
        specific_id = args.subscription_id or config.get('azure', {}).get('subscriptions', {}).get('specific_id')
        mg_id = args.management_group_id or config.get('azure', {}).get('subscriptions', {}).get('management_group', {}).get('id')
        mg_name = args.management_group_name or config.get('azure', {}).get('subscriptions', {}).get('management_group', {}).get('name')
        
        if mg_id or mg_name:
            subscriptions = get_management_group_subscriptions(credential, mg_id, mg_name)
            if not subscriptions:
                logger.error("No subscriptions found in the specified management group")
                sys.exit(1)
        elif specific_id:
            logger.info(f"Processing only subscription {specific_id}")
            sub_client = SubscriptionClient(credential)
            try:
                sub_details = sub_client.subscriptions.get(specific_id)
                real_display_name = sub_details.display_name
                logger.info(f"Retrieved real display name for subscription {specific_id}: {real_display_name}")
            except Exception as e:
                logger.warning(f"Could not retrieve display name for {specific_id}: {str(e)}. Using fallback.")
                real_display_name = f"Subscription {specific_id}"
            subscriptions = [type('obj', (object,), {
                'subscription_id': specific_id,
                'display_name': real_display_name
            })]
        elif process_all:
            subscriptions = get_azure_subscriptions(credential)
        else:
            logger.error("No subscription processing method specified")
            sys.exit(1)
        
        all_network_data = []
        
        for subscription in subscriptions:
            subscription_id = subscription.subscription_id
            subscription_name = subscription.display_name

            # --- SKIP "DELETED" SUBSCRIPTIONS ---
            if "DELETED" in subscription_name.upper():
                logger.info(f"Skipping subscription '{subscription_name}' ({subscription_id}) because it is marked as DELETED.")
                continue
            # ------------------------------------

            subscription_data = {
                'subscription_id': subscription_id,
                'subscription_name': subscription_name,
                'vnets': []
            }
            
            vnets_data = get_vnets_and_subnets(subscription_id, credential)
            subscription_data['vnets'] = vnets_data
            all_network_data.append(subscription_data)
        
        sync_to_netbox(all_network_data, netbox_url, netbox_token, config, credential)
        
        logger.info("Azure to Netbox sync completed successfully")
        
    except Exception as e:
        logger.error(f"Error during Azure to Netbox sync: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()
```
